{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe audio using a locally-running HuggingFace Model\n",
    "3/24/2025, Dave Sisk, https://github.com/davidcsisk, https://www.linkedin.com/in/davesisk-doctordatabase/\n",
    "\n",
    "These Vosk transcription models can be and are run locally. We avoid the charges for calls to OpenAI's Whisper model as well as avoid shipping data outside the confines of the host running this process, but we do need sufficient computing power to run the process locally, of course.  The smaller models will work on laptops and mobile devices, the larger Vosk models will need more fully provisioned resources. Details about the Vosk models can be found here: https://github.com/alphacep/vosk-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the necessary prep work...\n",
    "#! pip install vosk\n",
    "# Need to download and extract Vosk model into ./model directory\n",
    "# https://alphacephei.com/vosk/models\n",
    "# The model director should be in the current working directory,\n",
    "# and look like something like this:\n",
    "# model/\n",
    "# ├── am/                # Acoustic model files\n",
    "# ├── conf/              # Configuration files\n",
    "# ├── graph/             # Graph files for decoding\n",
    "# ├── ivector/           # Files for speaker identification (optional)\n",
    "# ├── rescore/           # Rescoring files (optional)\n",
    "# ├── README             # Information about the model\n",
    "# └── other files        # Additional files required for the model\n",
    "\n",
    "#! pip install wave\n",
    "#! pip install pyaudio\n",
    "#! pip install pydub\n",
    "\n",
    "# Must download and install ffmpeg\n",
    "# https://ffmpeg.org/download.html\n",
    "# check it's accessible in the path: ffmpeg -version  (You might have to restart terminal/vscode/jupyter notebook kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Vosk model\n",
    "#model = Model(\"vosk-model-small-en-us-0.15\")  # Ensure the Vosk model is downloaded and placed in the \"model\" directory\n",
    "model = Model(\"vosk-model-en-us-0.22-lgraph\")  # Ensure the Vosk model is downloaded and placed in the \"model\" directory\n",
    "\n",
    "# Directory containing MP3 files\n",
    "audio_directory = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert MP3 to WAV with required specifications\n",
    "def convert_mp3_to_wav(mp3_file, wav_file):\n",
    "    from pydub import AudioSegment\n",
    "    audio = AudioSegment.from_mp3(mp3_file)\n",
    "    # Convert to mono, 16-bit, and 16kHz\n",
    "    audio = audio.set_channels(1).set_sample_width(2).set_frame_rate(16000)\n",
    "    audio.export(wav_file, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing popularhistoryoftheartofmusic_00_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_00_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_01_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_00_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_01_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_01_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_02_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_01_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_02_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_02_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_02_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.txt\n",
      "Transcription saved to popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert and transcibe all the mp3 files...this process takes around 17m, and seems to be the same \n",
    "# whether we use the smallest model or the 2nd smallest (the difference is likely memory usage instead of cpu)\n",
    "# Iterate through all MP3 files in the directory\n",
    "for file_name in os.listdir(audio_directory):\n",
    "    if file_name.endswith(\".mp3\"):\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        wav_file = f\"{os.path.splitext(file_name)[0]}.wav\"\n",
    "        \n",
    "        # Convert MP3 to WAV\n",
    "        convert_mp3_to_wav(file_name, wav_file)\n",
    "        \n",
    "        # Open the WAV file\n",
    "        with wave.open(wav_file, \"rb\") as wf:\n",
    "            if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() not in [8000, 16000]:\n",
    "                print(f\"Skipping {file_name}: WAV file must be mono, 16-bit, and 8kHz or 16kHz.\")\n",
    "                continue\n",
    "            \n",
    "            rec = KaldiRecognizer(model, wf.getframerate())\n",
    "            transcription = \"\"\n",
    "            \n",
    "            # Perform transcription\n",
    "            while True:\n",
    "                data = wf.readframes(4000)\n",
    "                if len(data) == 0:\n",
    "                    break\n",
    "                if rec.AcceptWaveform(data):\n",
    "                    result = json.loads(rec.Result())\n",
    "                    transcription += result.get(\"text\", \"\") + \" \"\n",
    "            \n",
    "            # Finalize transcription\n",
    "            final_result = json.loads(rec.FinalResult())\n",
    "            transcription += final_result.get(\"text\", \"\")\n",
    "            \n",
    "            # Save the transcription to a text file\n",
    "            output_file = f\"{os.path.splitext(file_name)[0]}.txt\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(transcription.strip())\n",
    "            print(f\"Transcription saved to {output_file}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the 2nd smallest model (the one with an embedded language graph) produces a little higher quality / more accurate output than the smallest model, so we'll base this work off of that model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downstream, the intention is to use this process along with vector embeddings to try to detect deepfake audio that is used in exploits such as the CEO funds transfer scam and similar...it seems much easier to get accurate and meaningful vector embeddings if we first convert the raw audio to transcribed text. Look for an example of that in the vector search/cybersecurity project also owned by me. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an example that works from a video source.  Deepfake tools generally produce video output, not just audio. Because the Vosk model only understands audio data, we'll need to first extract audio from the video, then extract text transcription from that audio. This might not be necessary with a different model...it depends on the model being used to do the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extracted and saved as ./HomerSimpson-deepfake-video.wav\n",
      "Transcription:\n",
      "alright i am following up on the audit readiness review air and q one expenses for contractor payment we need to transfer one hundred and thirty seven thousand eight hundred and twenty dollars zero sense to the holding account a b c twelve thousand three hundred and forty five this must be sent by three p m t day so r c f epo can refer elected in the pre audit submission let me know once the transfer is done thanks\n",
      "Transcription saved to ./HomerSimpson-deepfake-video.txt\n"
     ]
    }
   ],
   "source": [
    "# Example of video -> audio -> transcribed text\n",
    "import subprocess\n",
    "import os\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "# Function to extract audio from MP4 and convert to WAV\n",
    "def extract_audio_from_mp4(mp4_file, wav_file):\n",
    "    # Use ffmpeg to extract audio and convert to WAV\n",
    "    command = [\n",
    "        \"ffmpeg\", \"-i\", mp4_file, \"-ac\", \"1\", \"-ar\", \"16000\", \"-sample_fmt\", \"s16\", wav_file\n",
    "    ]\n",
    "    try:\n",
    "        # Add a timeout to prevent hanging\n",
    "        subprocess.run(command, check=True, timeout=60)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        raise TimeoutError(f\"FFmpeg process timed out while processing {mp4_file}.\")\n",
    "\n",
    "    # Verify the WAV file was created and is not empty\n",
    "    if not os.path.exists(wav_file):\n",
    "        raise FileNotFoundError(f\"WAV file {wav_file} was not created.\")\n",
    "    if os.path.getsize(wav_file) == 0:\n",
    "        raise ValueError(f\"WAV file {wav_file} is empty.\")\n",
    "\n",
    "# Function to transcribe text from WAV audio\n",
    "def transcribe_audio(wav_file, model):\n",
    "    with wave.open(wav_file, \"rb\") as wf:\n",
    "        if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() not in [8000, 16000]:\n",
    "            raise ValueError(\"WAV file must be mono, 16-bit, and 8kHz or 16kHz.\")\n",
    "\n",
    "        rec = KaldiRecognizer(model, wf.getframerate())\n",
    "        transcription = \"\"\n",
    "\n",
    "        # Perform transcription\n",
    "        while True:\n",
    "            data = wf.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                result = json.loads(rec.Result())\n",
    "                transcription += result.get(\"text\", \"\") + \" \"\n",
    "\n",
    "        # Finalize transcription\n",
    "        final_result = json.loads(rec.FinalResult())\n",
    "        transcription += final_result.get(\"text\", \"\")\n",
    "\n",
    "        return transcription.strip()\n",
    "\n",
    "# Example usage\n",
    "mp4_file = \"./HomerSimpson-deepfake-video.mp4\"  # Replace with your MP4 file\n",
    "wav_file = f\"{os.path.splitext(mp4_file)[0]}.wav\"\n",
    "\n",
    "# Delete existing WAV file if it exists\n",
    "if os.path.exists(wav_file):\n",
    "    os.remove(wav_file)\n",
    "\n",
    "# Extract audio\n",
    "extract_audio_from_mp4(mp4_file, wav_file)\n",
    "print(f\"Audio extracted and saved as {wav_file}\")\n",
    "\n",
    "# Load Vosk model\n",
    "model = Model(\"vosk-model-en-us-0.22-lgraph\")\n",
    "\n",
    "# Transcribe audio\n",
    "transcription = transcribe_audio(wav_file, model)\n",
    "print(\"Transcription:\")\n",
    "print(transcription)\n",
    "\n",
    "# Save transcription to a text file\n",
    "output_file = f\"{os.path.splitext(mp4_file)[0]}.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(transcription)\n",
    "print(f\"Transcription saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
