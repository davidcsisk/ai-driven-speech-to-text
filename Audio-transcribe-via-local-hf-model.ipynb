{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe audio using a locally-running HuggingFace Model\n",
    "3/24/2025, Dave Sisk, https://github.com/davidcsisk, https://www.linkedin.com/in/davesisk-doctordatabase/\n",
    "\n",
    "These Vosk transcription models can be and are run locally. We avoid the charges for calls to OpenAI's Whisper model as well as avoid shipping data outside the confines of the host running this process, but we do need sufficient computing power to run the process locally, of course.  The smaller models will work on laptops and mobile devices, the larger Vosk models will need more fully provisioned resources. Details about the Vosk models can be found here: https://github.com/alphacep/vosk-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the necessary prep work...\n",
    "#! pip install vosk\n",
    "# Need to download and extract Vosk model into ./model directory\n",
    "# https://alphacephei.com/vosk/models\n",
    "# The model director should be in the current working directory,\n",
    "# and look like something like this:\n",
    "# model/\n",
    "# ├── am/                # Acoustic model files\n",
    "# ├── conf/              # Configuration files\n",
    "# ├── graph/             # Graph files for decoding\n",
    "# ├── ivector/           # Files for speaker identification (optional)\n",
    "# ├── rescore/           # Rescoring files (optional)\n",
    "# ├── README             # Information about the model\n",
    "# └── other files        # Additional files required for the model\n",
    "\n",
    "#! pip install wave\n",
    "#! pip install pyaudio\n",
    "#! pip install pydub\n",
    "\n",
    "# Must download and install ffmpeg\n",
    "# https://ffmpeg.org/download.html\n",
    "# check it's accessible in the path: ffmpeg -version  (You might have to restart terminal/vscode/jupyter notebook kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Vosk model\n",
    "#model = Model(\"vosk-model-small-en-us-0.15\")  # Ensure the Vosk model is downloaded and placed in the \"model\" directory\n",
    "model = Model(\"vosk-model-en-us-0.22-lgraph\")  # Ensure the Vosk model is downloaded and placed in the \"model\" directory\n",
    "\n",
    "# Directory containing MP3 files\n",
    "audio_directory = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert MP3 to WAV with required specifications\n",
    "def convert_mp3_to_wav(mp3_file, wav_file):\n",
    "    from pydub import AudioSegment\n",
    "    audio = AudioSegment.from_mp3(mp3_file)\n",
    "    # Convert to mono, 16-bit, and 16kHz\n",
    "    audio = audio.set_channels(1).set_sample_width(2).set_frame_rate(16000)\n",
    "    audio.export(wav_file, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing popularhistoryoftheartofmusic_00_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_00_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_01_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_00_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_01_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_01_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_02_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_01_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_02_mathews.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_02_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_02_mathews.txt\n",
      "Processing popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.mp3...\n",
      "Transcription saved to popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.txt\n",
      "Transcription saved to popularhistoryoftheartofmusic_03_mathews_reduced-sample-rate_16KHz.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert and transcibe all the mp3 files...this process takes around 17m, and seems to be the same \n",
    "# whether we use the smallest model or the 2nd smallest (the difference is likely memory usage instead of cpu)\n",
    "# Iterate through all MP3 files in the directory\n",
    "for file_name in os.listdir(audio_directory):\n",
    "    if file_name.endswith(\".mp3\"):\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        wav_file = f\"{os.path.splitext(file_name)[0]}.wav\"\n",
    "        \n",
    "        # Convert MP3 to WAV\n",
    "        convert_mp3_to_wav(file_name, wav_file)\n",
    "        \n",
    "        # Open the WAV file\n",
    "        with wave.open(wav_file, \"rb\") as wf:\n",
    "            if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() not in [8000, 16000]:\n",
    "                print(f\"Skipping {file_name}: WAV file must be mono, 16-bit, and 8kHz or 16kHz.\")\n",
    "                continue\n",
    "            \n",
    "            rec = KaldiRecognizer(model, wf.getframerate())\n",
    "            transcription = \"\"\n",
    "            \n",
    "            # Perform transcription\n",
    "            while True:\n",
    "                data = wf.readframes(4000)\n",
    "                if len(data) == 0:\n",
    "                    break\n",
    "                if rec.AcceptWaveform(data):\n",
    "                    result = json.loads(rec.Result())\n",
    "                    transcription += result.get(\"text\", \"\") + \" \"\n",
    "            \n",
    "            # Finalize transcription\n",
    "            final_result = json.loads(rec.FinalResult())\n",
    "            transcription += final_result.get(\"text\", \"\")\n",
    "            \n",
    "            # Save the transcription to a text file\n",
    "            output_file = f\"{os.path.splitext(file_name)[0]}.txt\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(transcription.strip())\n",
    "            print(f\"Transcription saved to {output_file}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the 2nd smallest model (the one with an embedded language graph) produces a little higher quality / more accurate output than the smallest model, so we'll base this work off of that model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downstream, the intention is to use this process along with vector embeddings to try to detect deepfake audio that is used in exploits such as the CEO funds transfer scam and similar...it seems much easier to get accurate and meaningful vector embeddings if we first convert the raw audio to transcribed text. Look for an example of that in the vector search/cybersecurity project also owned by me. Enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
